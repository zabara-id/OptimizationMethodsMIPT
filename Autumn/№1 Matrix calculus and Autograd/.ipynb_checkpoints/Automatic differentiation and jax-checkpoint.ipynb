{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6977e6ef-9196-4401-a441-b8e8a56e5e05",
   "metadata": {},
   "source": [
    "# Automatic differentiation and jax (part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5703ec13-f246-466c-a16c-68babb4630a5",
   "metadata": {},
   "source": [
    "## №1\n",
    "#### Работаем с функцией $f(x,y) = e^{-(sin(x)-cos(y))^2}$. Нарисуем вычислительный граф для этой функции, содержащий только примитивные операции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "387a9845-2311-4ff7-b111-80c6f151a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import numpy, grad\n",
    "\n",
    "\n",
    "# задаём функцию двух перменнных\n",
    "def func(xy):\n",
    "    return numpy.exp(-(numpy.sin(xy[0])-numpy.cos(xy[1]))**2)\n",
    "\n",
    "\n",
    "# берём её градиент, пользуясь функцией из jax\n",
    "def dexp(xy):\n",
    "    return grad(func)(xy)\n",
    "\n",
    "# создаём XLA представление функции dexp на входном массиве из 2 единиц\n",
    "z=jax.xla_computation(dexp)(numpy.ones(2))\n",
    "\n",
    "\n",
    "# создаём текстовый файл с HLO\n",
    "with open(\"t.txt\", \"w\") as f:\n",
    "    f.write(z.as_hlo_text())\n",
    "\n",
    "# создаём дамп в виде точеченого графика\n",
    "with open(\"t.dot\", \"w\") as f:\n",
    "    f.write(z.as_hlo_dot_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4598548-e04c-4ac7-90e4-20f6a0d07893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# запускаем dot\n",
    "!\"C:\\Program Files\\Graphviz\\bin\\dot.exe\" t.dot  -Tpng > t.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9115535b-4b40-4370-b482-3d98bb29a449",
   "metadata": {},
   "source": [
    "#### Вот сам граф:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e218746-d9ce-416e-9432-6da58b548735",
   "metadata": {},
   "source": [
    "![](t.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2454f382-18a9-4f93-a817-4b5699b67b5f",
   "metadata": {},
   "source": [
    "#### При решении задачи использовался источник, прикреплённый в качестве примера в условии задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e540dfd-eb86-4d57-a1aa-34b281a6ea22",
   "metadata": {},
   "source": [
    "## №2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d68563-e8c1-4ab5-918a-4ceb41d05c68",
   "metadata": {},
   "source": [
    "#### Найдём градиент функции $f(A) = tr(e^{A})$. Используем подход автоматического дифференцирования, библиотеку jax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b1dfa0b-952e-493e-8121-15f8800ae2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент f(A) с использванием JAX:\n",
      "[[1008.53906 1272.6573  2379.633  ]\n",
      " [1349.854   1706.0433  3187.2878 ]\n",
      " [ 465.00275  586.44183 1097.0242 ]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "from jax.scipy.linalg import expm\n",
    "\n",
    "\n",
    "# функция вычисления матричной экспоненты\n",
    "def expm(A: jnp.ndarray, n=50) -> jnp.ndarray:\n",
    "    dim = jnp.shape(A)[0]  # размерность квадратной матрицы\n",
    "    eA, temp = jnp.eye(dim), jnp.eye(dim)  # инициализация результата и промежуточной переменной единичной матрицей\n",
    "    \n",
    "    for i in range(1, n+1):\n",
    "        # вычисление текущего члена ряда\n",
    "        temp = jnp.dot(temp, A) / i\n",
    "         # добавление текущего члена к результату\n",
    "        eA += temp\n",
    "        \n",
    "    return eA\n",
    "\n",
    "\n",
    "def trace_expm(A):\n",
    "    exp_A = expm(A)\n",
    "    return jnp.trace(exp_A)\n",
    "\n",
    "\n",
    "# матрица для примера\n",
    "A = jnp.array([[1.0, 2.0, 2.0],\n",
    "               [3.0, 4.0, 1.0],\n",
    "               [7.0, 8.0, 1.0]])\n",
    "\n",
    "grad_trace_expm = grad(trace_expm)\n",
    "print('Градиент f(A) с использванием JAX:\\n', grad_trace_expm(A), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28485c87-01ab-4869-ac5d-65936d70dc08",
   "metadata": {},
   "source": [
    "#### Теперь надо данный результат сравнить с тем, что был получен аналитически в первой части задания. Аналитика показала, что градиент такой функции - это просто транспонированная экспонента матрицы. Убедимся в этом и тут:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "819ffdf9-5ea4-4ae9-a0b7-2c1027575580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент f(A) с использванием JAX:\n",
      "[[1008.53906 1272.6573  2379.633  ]\n",
      " [1349.854   1706.0433  3187.2878 ]\n",
      " [ 465.00275  586.44183 1097.0242 ]]\n",
      "\n",
      "Градиент f(A) согласно аналитическому выражению:\n",
      "[[1008.539   1272.657   2379.6326 ]\n",
      " [1349.8539  1706.0433  3187.2878 ]\n",
      " [ 465.00275  586.4419  1097.0242 ]]\n"
     ]
    }
   ],
   "source": [
    "# печатаем на экран градиент, полученный JAX-ом\n",
    "print('Градиент f(A) с использванием JAX:')\n",
    "print(grad_trace_expm(A), '\\n', sep='')\n",
    "\n",
    "# печатаем на экран градиент, полученный аналитически, то есть (e^A)^T\n",
    "print('Градиент f(A) согласно аналитическому выражению:')\n",
    "print(jnp.transpose(expm(A)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8052f93-0108-4bda-bb1e-6f30a1b9bfa7",
   "metadata": {},
   "source": [
    "#### Как видно, с высокой степенью точности градиент, полученный с помощью JAX совпадает с градиентом, полученным в первой части задания аналитически."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406155a-3179-4629-95f9-7e4ef1641424",
   "metadata": {},
   "source": [
    "## №5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3784a08a-bf9e-410d-aadc-dd70e4727f1f",
   "metadata": {},
   "source": [
    "#### Определим градиент функции $f(x)=x^{T}xx^{T}x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3d656db-0490-4f76-84b3-bfa8ebf6a1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент функции f(x), вычисленный с помощью JAX: [ 60.960003 121.920006 195.072   ]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "\n",
    "# определяем функцию f(x)\n",
    "def dot_square(x: jnp.ndarray) -> jnp.float64:\n",
    "    return jnp.dot(x, x) * jnp.dot(x, x)\n",
    "\n",
    "\n",
    "# находим градиент написанной функции на примере произвольного вектора ex\n",
    "ex = jnp.array([1, 2, 3.2])\n",
    "\n",
    "grad_dot_square = grad(dot_square)\n",
    "print(\"Градиент функции f(x), вычисленный с помощью JAX:\", grad_dot_square(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4807639-1ff4-498c-b556-77da86b2a4d6",
   "metadata": {},
   "source": [
    "#### Посчитаем теперь градиент функции вручную:\n",
    "#### $df=d(\\textlangle x, x\\rangle  \\textlangle x, x\\rangle)=2\\textlangle x, x\\rangle d(\\textlangle x, x\\rangle)=2\\textlangle x, x\\rangle\\textlangle 2x, dx\\rangle=\\textlangle 4x^{T}x*x, dx\\rangle\\Rightarrow\\boxed{\\nabla f(x)=4x^{T}x*x}$\n",
    "#### Сравним теперь полученный аналитически гражиент с тем, что был получен выше при помощи JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "abef7f51-b046-46b0-9b00-47493a72862b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент функции f(x), вычисленный с помощью JAX: [ 60.960003 121.920006 195.072   ]\n",
      "Градиент функции f(x), вычисленный аналитически: [ 60.960003 121.920006 195.072   ]\n"
     ]
    }
   ],
   "source": [
    "# задаём функцю аналитически полученного градиента\n",
    "def analytaical_grad(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    return 4 * jnp.dot(x, x) * x\n",
    "\n",
    "print(\"Градиент функции f(x), вычисленный с помощью JAX:\", grad_dot_square(ex))\n",
    "print(\"Градиент функции f(x), вычисленный аналитически:\", analytaical_grad(ex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470e094-be87-42ed-a8f1-93fe329e062d",
   "metadata": {},
   "source": [
    "#### Как видим, аналитический градиент и градиент, вычисленный с помощью JAX идентичны.\n",
    "#### Теперь найдём гессиан с помощью JAX. Для этого воспользуемся встроенной в бибиотеку функцией hessian():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97ff7c11-401d-425b-ad16-a53d66c1e6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гессиан функции f(x), вычисленный с помощью JAX:\n",
      "[[ 68.96001  16.       25.6    ]\n",
      " [ 16.       92.96001  51.2    ]\n",
      " [ 25.6      51.2     142.88   ]]\n"
     ]
    }
   ],
   "source": [
    "from jax import hessian\n",
    "\n",
    "hessian_dot_square = hessian(dot_square)\n",
    "jax_hessian = hessian_dot_square(ex)\n",
    "print(\"Гессиан функции f(x), вычисленный с помощью JAX:\\n\", jax_hessian, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2142df-9d5a-4164-8a44-56a95c67f03f",
   "metadata": {},
   "source": [
    "#### Найдём теперь гессиаан аналитически. Для этого нужно вычислить второй дифференциал, приняв в первом $dx\\equiv dx_1=const$.\n",
    "#### $d^{2}f=d(df)=d(\\langle 4x^{T}xx, dx_1\\rangle)=4\\langle d(x^{T}xx), dx_1\\rangle+0$\n",
    "#### $d(x^{T}xx)=2x^{T}dxx+x^{T}xdx$\n",
    "#### $\\Rightarrow d^{2}f=4\\langle 2x^{T}dxx+x^{T}xdx, dx_1\\rangle=8\\langle x^{T}dxx, dx_1\\rangle+4\\langle x^{T}xdx,dx_1\\rangle=8\\langle xx^{T}dx_1, dx\\rangle+4\\langle x^{T}xdx, dx_1\\rangle=8\\langle xx^{T}dx_1, dx\\rangle+4\\langle Ix^{T}xdx_1, dx\\rangle=$\n",
    "#### $=4\\langle (8xx^{T}+4I)dx_1, dx\\rangle\\Rightarrow\\boxed{H_f=8xx^{T}+4I}$\n",
    "#### Теперь надо сравнить то, что было получено в этой ячейке, с тем, что было получено при помощи JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "35dc6d13-fc69-4210-8fe7-9a120e599d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гессиан функции f(x), вычисленный с помощью JAX:\n",
      "[[ 68.96001  16.       25.6    ]\n",
      " [ 16.       92.96001  51.2    ]\n",
      " [ 25.6      51.2     142.88   ]]\n",
      "\n",
      "Гессиан функции f(x), вычисленный аналитически:\n",
      "[[ 68.96001  16.       25.6    ]\n",
      " [ 16.       92.96001  51.2    ]\n",
      " [ 25.6      51.2     142.88   ]]\n"
     ]
    }
   ],
   "source": [
    "# создаём функцию вычисления гессиана по формуле из аналитического решения\n",
    "def analytucal_hessian_dot_square(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    return 8*jnp.outer(x, x) + 4*jnp.dot(x, x)*jnp.eye(jnp.shape(x)[0])\n",
    "    \n",
    "\n",
    "# сравниваем\n",
    "analytical_hessian = analytucal_hessian_dot_square(ex)\n",
    "print(\"Гессиан функции f(x), вычисленный с помощью JAX:\\n\", jax_hessian, '\\n', sep='')\n",
    "print(\"Гессиан функции f(x), вычисленный аналитически:\\n\", analytical_hessian, sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858f33b2-ab73-4d51-b2f9-19d3e8dd84c3",
   "metadata": {},
   "source": [
    "#### Как видно, выход один и тот же."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cfd966-cfe5-4d30-84f6-e1db3a1e53ef",
   "metadata": {},
   "source": [
    "## №4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c1174e-1dcd-4295-b468-64a075460d0f",
   "metadata": {},
   "source": [
    "#### Найдём градиент функции $f(X)=-ln(det(X))$ с помощью JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7a3741a7-7685-45f4-9968-58bf748284e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент функции f(x), полученный с помощью JAX:\n",
      "[[ 0.07017544  0.02631579 -0.28070176]\n",
      " [-0.22807017  0.28947368 -0.0877193 ]\n",
      " [-0.01754386 -0.13157895  0.07017544]]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad\n",
    "\n",
    "\n",
    "# задаём функцию f(x)\n",
    "def lndet(X:jnp.ndarray) -> jnp.ndarray:\n",
    "    detX = jnp.linalg.det(X)\n",
    "    return -jnp.log(detX)\n",
    "\n",
    "# находим градиент написанной функции на примере произвольной квадратной матрицы EX\n",
    "EX = jnp.array([[1., 2., 4.],\n",
    "                [4., 0., 1.],\n",
    "                [9., 8., 3.]])\n",
    "\n",
    "jax_lndet_grad = grad(lndet)\n",
    "print(\"Градиент функции f(x), полученный с помощью JAX:\\n\", jax_lndet_grad(EX), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1a8ac-341e-47cd-8ceb-295691e46acd",
   "metadata": {},
   "source": [
    "#### Теперь вычислим градиаент $f(X)=-ln(det(X))$ аналитически:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2cf7c5-cd87-4e4e-ae66-add8a65ecddc",
   "metadata": {},
   "source": [
    "#### $d=-d(ln(det(X)))=-\\frac{det(X\\langle X^{-T}, dX\\rangle}{det(X)}=-\\langle X^{-T}, dx\\rangle\\Rightarrow\\boxed{\\nabla f=-X^{-T}}$\n",
    "#### Теперь сравним на примере матрицы EX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09a2052a-9399-491e-a460-420442b42cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиент функции f(x), полученный с помощью JAX:\n",
      "[[ 0.07017544  0.02631579 -0.28070176]\n",
      " [-0.22807017  0.28947368 -0.0877193 ]\n",
      " [-0.01754386 -0.13157895  0.07017544]]\n",
      "\n",
      "Градиент функции f(x), полученный аналитически:\n",
      "[[ 0.07017544  0.02631579 -0.28070176]\n",
      " [-0.22807018  0.28947368 -0.0877193 ]\n",
      " [-0.01754386 -0.13157895  0.07017544]]\n"
     ]
    }
   ],
   "source": [
    "# задаём функцию аналитически найденной формулы для градиента\n",
    "def analytical_grad_lndet(X:jnp.ndarray) -> jnp.ndarray:\n",
    "    return -jnp.linalg.inv(X).T\n",
    "\n",
    "# сравниваем JAX и аналитику на примере матрцы EX\n",
    "print(\"Градиент функции f(x), полученный с помощью JAX:\\n\", jax_lndet_grad(EX), '\\n', sep='')\n",
    "print(\"Градиент функции f(x), полученный аналитически:\\n\", analytical_grad_lndet(EX), sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86161049-413a-4fe6-bbe1-c9b61e38e9c9",
   "metadata": {},
   "source": [
    "#### Как видно из выхода ячейки выше, градиенты совпадают."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25587111-ba5d-4b33-b82f-cc4a751d0f5e",
   "metadata": {},
   "source": [
    "## №3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c68c0-b816-4274-9158-ff15b3c51eec",
   "metadata": {},
   "source": [
    "#### Условие длинное, поэтому перейдём сразу к решению. Напишем сначала функцию градиентного спуска для функции $f(x)$. \n",
    "#### В нашем случае $f(x) = \\frac{1}{2}\\|x\\|^{2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e2e0bd-81d0-491c-a913-6ee36daa3a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Функция в начальной точке:\n",
      "f(x0) = 176.581\n",
      "Функция после десяти шагов градиентного спуска:\n",
      "f(x10) = 56.808\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "from jax import grad, random\n",
    "import jax.numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Определяем функцию, которую будем оптимизировать\n",
    "def f(x):\n",
    "    return 0.5 * np.linalg.norm(x)**2\n",
    "\n",
    "\n",
    "# Определяем метод градиентного спуска\n",
    "def gradient_descent(f, x0, alpha, num_steps):\n",
    "    x = x0 # начальное приближение\n",
    "    grad_f = grad(f) # вычисляем градиент функции\n",
    "    for i in range(num_steps): # выполняем шаги градиентного спуска\n",
    "        x = x - alpha[i] * grad_f(x) # делаем шаг в направлении антиградиента\n",
    "    return x\n",
    "\n",
    "\n",
    "# инициализируем параметры\n",
    "key = jax.random.PRNGKey(1701) # seed\n",
    "x0 = random.uniform(key, (1000, ))\n",
    "alpha0 = random.uniform(key, (10, ), maxval=0.1)\n",
    "\n",
    "\n",
    "print(\"Функция в начальной точке:\")\n",
    "print(f\"f(x0) = {f(x0):.3f}\")\n",
    "print(\"Функция после десяти шагов градиентного спуска:\")\n",
    "print(f\"f(x10) = {f(gradient_descent(f, x0, alpha0, num_steps=10)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ca46f2-3cd7-403f-befd-894aadf6b9bd",
   "metadata": {},
   "source": [
    "#### А теперь нам нужно оптимизировать нашу функцию по набору параметров $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77571796-306c-4753-b991-261f9335cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# целевая функция f(x10)\n",
    "def L(alpha):\n",
    "    return f(gradient_descent(f, x0, alpha, 10))\n",
    "\n",
    "\n",
    "# Определяем функцию для оптимизации шага градиентного спуска\n",
    "def optimize_alpha(alpha0, betta, num_steps):\n",
    "    alpha = alpha0 # начальное значение шага\n",
    "    grad_L = grad(L) # градиент функции потерь\n",
    "    for i in range(num_steps): # шаги градиентного спуска для оптимизации шага\n",
    "        alpha = alpha - betta * grad_L(alpha) # обновляем значение шага\n",
    "    return alpha # возвращаем оптимизированное значение шага\n",
    "\n",
    "\n",
    "betta = 0.005\n",
    "new_alpha = optimize_alpha(alpha0, betta, num_steps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c88c655-317c-4d76-9fa7-f590a9cc2a6f",
   "metadata": {},
   "source": [
    "#### Выше мы получили новый оптимизированный набор параметров $\\alpha$ и сохранили его в переменную new_alpha. Теперь посчитаем градиентный спуск на тех же 10-ти шагах для функции $f(x) = \\frac{1}{2}\\|x\\|^{2}$, но уже с новым оптимизированным набором параметров $\\alpha$. Сравним полученное значение со значением градиентного спуска, использующего старые параметры. Чтобы понять, что полученный набор new_alpha стал лучше, можно сравнивать значение функции $L=f(x_{10})$. Если значение функции уменьшится, значит набор $\\alpha$ стал лучше. Численно проверить оптимальность в данной задаче можно следующим образом: посчитать градиент L в точке, соответствующей текущему набору $\\alpha$. Если градиент близок к нулю (в пределах заданной точности), то можно считать, что найден локальный минимум. Но в данном случае и так очевидно, что у функции есть глобальный минимум равный нулю в точке $x^{*}=(0, 0, ..., 0)^{T}\\in\\mathbb{B}^{1000}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de98e777-5a4f-4402-88f8-e8141961b569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Функция в начальной точке:\n",
      "f(x0) = 176.581\n",
      "Функция после десяти шагов градиентного спуска со старыми парааметрами:\n",
      "f(x10) = 56.808\n",
      "Функция после десяти шагов градиентного спуска с новыми парааметрами:\n",
      "f(x10) = 7.869951446082268e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"Функция в начальной точке:\")\n",
    "print(f\"f(x0) = {f(x0):.3f}\")\n",
    "print(\"Функция после десяти шагов градиентного спуска со старыми парааметрами:\")\n",
    "print(f\"f(x10) = {f(gradient_descent(f, x0, alpha0, num_steps=10)):.3f}\")\n",
    "print(\"Функция после десяти шагов градиентного спуска с новыми парааметрами:\")\n",
    "print(f\"f(x10) = {f(gradient_descent(f, x0, new_alpha, num_steps=10))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e29079-470b-4c94-8281-5badc9dad13f",
   "metadata": {},
   "source": [
    "#### Видим, что с использованием новых оптимизированный параметров $\\alpha$ за те же 10 шагов градиентный спуск привёл нас к оптимуму намного ближе, чем раньше, что говорит о том, что новый набор параметров $\\alpha$ стал значительно лучше."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
